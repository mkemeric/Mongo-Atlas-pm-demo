{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d586f25",
   "metadata": {},
   "source": [
    "# Agent: Failure Prediction\n",
    "\n",
    "AI-Driven Predictive Maintenance with MongoDB Atlas\n",
    "\n",
    "Overview\n",
    "\n",
    "This notebook demonstrates an agentic workflow for failure prediction and root cause analysis in industrial settings, leveraging MongoDB Atlas as the central data platform.\n",
    "\n",
    "Automated Root Cause Analysis\n",
    "\n",
    "The end user sends an equipment malfunction alert to the agent\n",
    "\n",
    "Failure Agent performs diagnostics that traditionally take hours manually\n",
    "\n",
    "Atlas Vector Search retrieves contextual insights from:\n",
    "- Historical maintenance logs\n",
    "- Equipment documentation\n",
    "- Environmental data (temperature, humidity, etc.)\n",
    "= ERP/MES systems\n",
    "\n",
    "LLM analyzes gathered context and generates incident reports with corrective actions\n",
    "\n",
    "Architecture Flow\n",
    "\n",
    "Real-time Alerts → AI Agents → Root Cause Analysis → Incident Reports\n",
    "\n",
    "## Prerequisites\n",
    "- MongoDB Atlas cluster with Atlas Vector Search enabled\n",
    "- Voyage AI API key\n",
    "- OpenAI API key\n",
    "- Python packages: pymongo, voyageai, openai, langchain, langchain-openai, langgraph, asyncio, nest_asyncio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67cb3ef7",
   "metadata": {},
   "source": [
    "## 1: Import Required Libraries and Configure Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2963f1f3",
   "metadata": {},
   "source": [
    "## Setup: Configure Virtual Environment and Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a0d60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installing a libraries' directly in the notebook\n",
    "%pip install dotenv pymongo voyageai openai langchain langchain-openai langgraph asyncio nest_asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f95be3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import sys\n",
    "import httpx\n",
    "from pathlib import Path\n",
    "from typing import Optional, List, Dict, Any\n",
    "from pymongo import MongoClient\n",
    "from pymongo.errors import ServerSelectionTimeoutError\n",
    "from pymongo.operations import SearchIndexModel\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import voyageai\n",
    "from openai import OpenAI\n",
    "\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Configure MONGODB URI from environment variables\n",
    "MONGODB_URI = os.getenv(\"MONGODB_URI\")\n",
    "DATABASE_NAME = os.getenv(\"DATABASE_NAME\")\n",
    "\n",
    "# Configure LLM endpoint and API keys from environment variables\n",
    "LLM_API_ENDPOINT = os.getenv(\"LLM_API_ENDPOINT\")\n",
    "LLM_API_KEY = os.getenv(\"LLM_API_KEY\")\n",
    "COMPLETION_MODEL = os.getenv(\"COMPLETION_MODEL\", \"gpt-3.5-turbo\")  # Default to gpt-3.5-turbo if not set\n",
    "\n",
    "# Configure embedding API settings\n",
    "# Set USE_OPENAI_COMPATIBLE_EMBEDDINGS to 'true' or '1' to use OpenAI-compatible API\n",
    "# Set to 'false' or '0' (or leave unset) to use Voyage AI native API\n",
    "USE_OPENAI_COMPATIBLE_EMBEDDINGS = os.getenv(\"USE_OPENAI_COMPATIBLE_EMBEDDINGS\", \"false\").lower() in ['true', '1', 'yes']\n",
    "\n",
    "# Embedding API configuration\n",
    "EMBEDDING_API_ENDPOINT = os.getenv(\"EMBEDDING_API_ENDPOINT\")  # Used for OpenAI-compatible API\n",
    "EMBEDDING_API_KEY = os.getenv(\"EMBEDDING_API_KEY\")\n",
    "EMBEDDING_MODEL = os.getenv(\"EMBEDDING_MODEL\")\n",
    "\n",
    "# Legacy Voyage-specific variables (for backward compatibility)\n",
    "VOYAGE_API_ENDPOINT = os.getenv(\"VOYAGE_API_ENDPOINT\")\n",
    "VOYAGE_API_KEY = os.getenv(\"VOYAGE_API_KEY\")\n",
    "\n",
    "# If using Voyage native API and EMBEDDING_API_KEY not set, use VOYAGE_API_KEY\n",
    "if not USE_OPENAI_COMPATIBLE_EMBEDDINGS and not EMBEDDING_API_KEY and VOYAGE_API_KEY:\n",
    "    EMBEDDING_API_KEY = VOYAGE_API_KEY\n",
    "\n",
    "# SSL verification setting (for internal/dev environments with self-signed certs)\n",
    "DISABLE_SSL_VERIFICATION = os.getenv(\"DISABLE_SSL_VERIFICATION\", \"false\").lower() in ['true', '1', 'yes']\n",
    "\n",
    "# Validate that API keys are available\n",
    "print(\"Configuration Status:\")\n",
    "print(f\"✓ MongoDB URI configured: {bool(MONGODB_URI)}\")\n",
    "print(f\"✓ Embedding API type: {'OpenAI-compatible' if USE_OPENAI_COMPATIBLE_EMBEDDINGS else 'Voyage AI native'}\")\n",
    "print(f\"✓ Embedding API key configured: {bool(EMBEDDING_API_KEY)}\")\n",
    "print(f\"✓ Embedding model configured: {bool(EMBEDDING_MODEL)}\")\n",
    "print(f\"✓ LLM API key configured: {bool(LLM_API_KEY)}\")\n",
    "print(f\"✓ LLM completion model: {COMPLETION_MODEL}\")\n",
    "\n",
    "# Initialize embedding client based on configuration\n",
    "embedding_client = None\n",
    "\n",
    "if USE_OPENAI_COMPATIBLE_EMBEDDINGS:\n",
    "    # Use OpenAI-compatible API (e.g., MLIS, local OpenAI-compatible server, etc.)\n",
    "    if EMBEDDING_API_KEY:\n",
    "        # Create HTTP client with optional SSL verification disabled\n",
    "        if DISABLE_SSL_VERIFICATION:\n",
    "            http_client = httpx.Client(verify=False)\n",
    "            print(\"⚠ WARNING: SSL verification disabled for embedding client\")\n",
    "        else:\n",
    "            http_client = None\n",
    "        \n",
    "        embedding_client = OpenAI(\n",
    "            api_key=EMBEDDING_API_KEY,\n",
    "            base_url=EMBEDDING_API_ENDPOINT,\n",
    "            http_client=http_client\n",
    "        )\n",
    "        print(f\"✓ Initialized OpenAI-compatible embedding client (endpoint: {EMBEDDING_API_ENDPOINT})\")\n",
    "    else:\n",
    "        print(\"✗ EMBEDDING_API_KEY not configured for OpenAI-compatible API\")\n",
    "else:\n",
    "    # Use Voyage AI native API\n",
    "    if EMBEDDING_API_KEY:\n",
    "        embedding_client = voyageai.Client(api_key=EMBEDDING_API_KEY)\n",
    "        print(f\"✓ Initialized Voyage AI native client\")\n",
    "    else:\n",
    "        print(\"✗ EMBEDDING_API_KEY (or VOYAGE_API_KEY) not configured for Voyage AI\")\n",
    "\n",
    "# Initialize LLM client\n",
    "if LLM_API_KEY:\n",
    "    # Create HTTP client with optional SSL verification disabled\n",
    "    if DISABLE_SSL_VERIFICATION:\n",
    "        llm_http_client = httpx.Client(verify=False)\n",
    "        print(\"⚠ WARNING: SSL verification disabled for LLM client\")\n",
    "    else:\n",
    "        llm_http_client = None\n",
    "    \n",
    "    llm_client = OpenAI(\n",
    "        api_key=LLM_API_KEY,\n",
    "        base_url=LLM_API_ENDPOINT if LLM_API_ENDPOINT else None,\n",
    "        http_client=llm_http_client\n",
    "    )\n",
    "    print(f\"✓ Initialized LLM client\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e978da6f",
   "metadata": {},
   "source": [
    "## 2: Connect to MongoDB Atlas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c83011",
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect_to_mongodb(uri: str, db_name: str = DATABASE_NAME) -> tuple:\n",
    "    \"\"\"\n",
    "    Connect to MongoDB Atlas cluster\n",
    "    \n",
    "    Args:\n",
    "        uri: MongoDB connection string\n",
    "        db_name: Database name to use\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (client, database)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        client = MongoClient(uri, serverSelectionTimeoutMS=5000)\n",
    "        # Verify connection\n",
    "        client.admin.command('ping')\n",
    "        db = client[db_name]\n",
    "        print(f\"✓ Successfully connected to MongoDB Atlas\")\n",
    "        print(f\"✓ Database: {db_name}\")\n",
    "        return client, db\n",
    "    except ServerSelectionTimeoutError:\n",
    "        print(\"✗ Failed to connect to MongoDB Atlas. Check your connection string.\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Connection error: {e}\")\n",
    "        raise\n",
    "\n",
    "# Connect to MongoDB\n",
    "if MONGODB_URI:\n",
    "    mongo_client, db = connect_to_mongodb(MONGODB_URI)\n",
    "    print(f\"✓ Available collections: {db.list_collection_names()}\")\n",
    "else:\n",
    "    print(\"✗ MONGODB_URI not configured. Please set the environment variable.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21fd92e9",
   "metadata": {},
   "source": [
    "## 3: Load and Ingest Datasets from Data Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8fb025c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json_dataset(file_path: str) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Load a JSON dataset from file\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to the JSON file\n",
    "        \n",
    "    Returns:\n",
    "        List of documents\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        print(f\"✓ Loaded {file_path}: {len(data)} documents\")\n",
    "        return data if isinstance(data, list) else [data]\n",
    "    except FileNotFoundError:\n",
    "        print(f\"✗ File not found: {file_path}\")\n",
    "        return []\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"✗ Error decoding JSON from {file_path}: {e}\")\n",
    "        return []\n",
    "\n",
    "# Locate the data folder\n",
    "data_folder = Path(\"./data\")\n",
    "if not data_folder.exists():\n",
    "    print(f\"✗ Data folder not found at {data_folder.absolute()}\")\n",
    "    # Try alternative path\n",
    "    alt_path = Path(\"../data\")\n",
    "    if alt_path.exists():\n",
    "        data_folder = alt_path\n",
    "    else:\n",
    "        print(\"Please ensure the data folder exists in the workspace root\")\n",
    "\n",
    "print(f\"\\nLoading datasets from: {data_folder.absolute()}\")\n",
    "print(f\"Available files: {list(data_folder.glob('*'))}\\n\")\n",
    "\n",
    "# Load datasets\n",
    "manuals_data = load_json_dataset(str(data_folder / \"manuals.json\"))\n",
    "interviews_data = load_json_dataset(str(data_folder / \"interviews.json\"))\n",
    "workorders_data = load_json_dataset(str(data_folder / \"workorders.json\"))\n",
    "\n",
    "print(f\"\\nDataset Summary:\")\n",
    "print(f\"  - Manuals: {len(manuals_data)} documents\")\n",
    "print(f\"  - Interviews: {len(interviews_data)} documents\")\n",
    "print(f\"  - Work Orders: {len(workorders_data)} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b15bb686",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ingest_data_to_mongodb(db, collection_name: str, documents: List[Dict]) -> bool:\n",
    "    \"\"\"\n",
    "    Ingest documents into a MongoDB collection\n",
    "    \n",
    "    Args:\n",
    "        db: MongoDB database object\n",
    "        collection_name: Name of the collection\n",
    "        documents: List of documents to insert\n",
    "        \n",
    "    Returns:\n",
    "        True if successful, False otherwise\n",
    "    \"\"\"\n",
    "    if not documents:\n",
    "        print(f\"✗ No documents to ingest into {collection_name}\")\n",
    "        return False\n",
    "    \n",
    "    try:\n",
    "        collection = db[collection_name]\n",
    "        # Drop existing collection to start fresh\n",
    "        collection.drop()\n",
    "        \n",
    "        # Insert documents\n",
    "        result = collection.insert_many(documents)\n",
    "        print(f\"✓ Ingested {len(result.inserted_ids)} documents into '{collection_name}'\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error ingesting data into {collection_name}: {e}\")\n",
    "        return False\n",
    "\n",
    "# Ingest datasets into MongoDB\n",
    "if MONGODB_URI and manuals_data:\n",
    "    ingest_data_to_mongodb(db, \"manuals\", manuals_data)\n",
    "    \n",
    "if MONGODB_URI and interviews_data:\n",
    "    ingest_data_to_mongodb(db, \"interviews\", interviews_data)\n",
    "\n",
    "if MONGODB_URI and workorders_data:\n",
    "    ingest_data_to_mongodb(db, \"workorders\", workorders_data)\n",
    "\n",
    "print(\"\\n✓ Data ingestion complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "720d4fb2",
   "metadata": {},
   "source": [
    "## 4: Generate Embeddings Using Configured Model\n",
    "\n",
    "Embeddings are generated using the model specified in your `.env` file.\n",
    "\n",
    "The notebook supports both:\n",
    "- **OpenAI-compatible APIs** (HPE MLIS - recommended): NVIDIA, Voyage, custom models\n",
    "- **Voyage AI native API** (legacy): For Voyage cloud service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8209481",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_for_embedding(document: Dict[str, Any], text_fields: List[str] = None) -> str:\n",
    "    \"\"\"\n",
    "    Extract text content from a document for embedding\n",
    "    \n",
    "    Args:\n",
    "        document: The document to extract text from\n",
    "        text_fields: List of field names to extract (if None, uses sensible defaults)\n",
    "        \n",
    "    Returns:\n",
    "        Combined text string\n",
    "    \"\"\"\n",
    "    if text_fields is None:\n",
    "        # Default fields to check for text content\n",
    "        text_fields = ['text', 'title', 'observations']\n",
    "    \n",
    "    texts = []\n",
    "    for field in text_fields:\n",
    "        if field in document and document[field]:\n",
    "            value = document[field]\n",
    "            if isinstance(value, str):\n",
    "                texts.append(value)\n",
    "            elif isinstance(value, list):\n",
    "                texts.extend([str(v) for v in value if v])\n",
    "    \n",
    "    return \" \".join(texts)\n",
    "\n",
    "def generate_embeddings_batch(texts: List[str], model: str = EMBEDDING_MODEL, input_type: str = \"passage\") -> List[List[float]]:\n",
    "    \"\"\"\n",
    "    Generate embeddings for a batch of texts using configured API\n",
    "    \n",
    "    Supports both Voyage AI native API and OpenAI-compatible APIs.\n",
    "    The API type is determined by USE_OPENAI_COMPATIBLE_EMBEDDINGS variable.\n",
    "    \n",
    "    Args:\n",
    "        texts: List of texts to embed\n",
    "        model: Model name to use for embeddings\n",
    "        input_type: Type of input - \"passage\" for documents, \"query\" for search queries\n",
    "        \n",
    "    Returns:\n",
    "        List of embedding vectors\n",
    "    \"\"\"\n",
    "    if not texts:\n",
    "        return []\n",
    "    \n",
    "    if not embedding_client:\n",
    "        print(\"✗ Embedding client not initialized\")\n",
    "        return []\n",
    "    \n",
    "    try:\n",
    "        if USE_OPENAI_COMPATIBLE_EMBEDDINGS:\n",
    "            # Use OpenAI-compatible API\n",
    "            # Some endpoints (like Voyage models on MLIS) require input_type parameter\n",
    "            response = embedding_client.embeddings.create(\n",
    "                input=texts,\n",
    "                model=model,\n",
    "                extra_body={\"input_type\": input_type}  # passage for docs, query for searches\n",
    "            )\n",
    "            embeddings = [e.embedding for e in response.data]\n",
    "        else:\n",
    "            # Use Voyage AI native API\n",
    "            # Map input_type: passage->document, query->query\n",
    "            voyage_input_type = \"document\" if input_type == \"passage\" else \"query\"\n",
    "            response = embedding_client.embed(\n",
    "                texts=texts,\n",
    "                model=model,\n",
    "                input_type=voyage_input_type\n",
    "            )\n",
    "            embeddings = [e for e in response.embeddings]\n",
    "        \n",
    "        print(f\"✓ Generated {len(embeddings)} embeddings using {model}\")\n",
    "        return embeddings\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error generating embeddings: {e}\")\n",
    "        return []\n",
    "\n",
    "# Test embedding generation with a sample\n",
    "print(f\"\\nTesting embedding generation ({'OpenAI-compatible' if USE_OPENAI_COMPATIBLE_EMBEDDINGS else 'Voyage AI native'})...\")\n",
    "test_texts = [\"This is a test document\", \"Another test text for embeddings\"]\n",
    "test_embeddings = generate_embeddings_batch(test_texts)\n",
    "if test_embeddings:\n",
    "    print(f\"✓ Sample embedding dimension: {len(test_embeddings[0])}\")\n",
    "else:\n",
    "    print(\"✗ Embedding generation failed. Check API key and connectivity.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b2b54e",
   "metadata": {},
   "source": [
    "## 5: Update Collections with Embeddings Field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1292c9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_embeddings_to_collection(db, collection_name: str, batch_size: int = 10) -> bool:\n",
    "    \"\"\"\n",
    "    Generate embeddings for all documents in a collection and update them\n",
    "    \n",
    "    Args:\n",
    "        db: MongoDB database object\n",
    "        collection_name: Name of the collection to process\n",
    "        batch_size: Number of documents to process per batch\n",
    "        \n",
    "    Returns:\n",
    "        True if successful, False otherwise\n",
    "    \"\"\"\n",
    "    try:\n",
    "        collection = db[collection_name]\n",
    "        documents = list(collection.find({}))\n",
    "        \n",
    "        if not documents:\n",
    "            print(f\"✗ No documents found in collection '{collection_name}'\")\n",
    "            return False\n",
    "        \n",
    "        print(f\"\\nProcessing {len(documents)} documents in '{collection_name}'...\")\n",
    "        \n",
    "        # Process documents in batches\n",
    "        for i in range(0, len(documents), batch_size):\n",
    "            batch = documents[i:i + batch_size]\n",
    "            \n",
    "            # Extract text from documents\n",
    "            texts = [extract_text_for_embedding(doc) for doc in batch]\n",
    "            \n",
    "            # Generate embeddings for the batch\n",
    "            embeddings = generate_embeddings_batch(texts)\n",
    "            \n",
    "            if not embeddings or len(embeddings) != len(batch):\n",
    "                print(f\"✗ Embedding generation failed for batch {i//batch_size + 1}\")\n",
    "                continue\n",
    "            \n",
    "            # Update documents with embeddings\n",
    "            for doc, embedding in zip(batch, embeddings):\n",
    "                collection.update_one(\n",
    "                    {\"_id\": doc[\"_id\"]},\n",
    "                    {\"$set\": {\"embeddings\": embedding}}\n",
    "                )\n",
    "            \n",
    "            print(f\"  ✓ Processed batch {i//batch_size + 1}/{(len(documents) + batch_size - 1)//batch_size}\")\n",
    "        \n",
    "        # Verify embeddings were added\n",
    "        docs_with_embeddings = collection.count_documents({\"embeddings\": {\"$exists\": True}})\n",
    "        print(f\"✓ Updated {docs_with_embeddings} documents with embeddings in '{collection_name}'\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error adding embeddings to {collection_name}: {e}\")\n",
    "        return False\n",
    "\n",
    "# Add embeddings to both collections\n",
    "if MONGODB_URI and test_embeddings:  # Only proceed if embeddings work\n",
    "    print(\"=\" * 60)\n",
    "    print(\"GENERATING AND ADDING EMBEDDINGS TO COLLECTIONS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    add_embeddings_to_collection(db, \"manuals\", batch_size=5)\n",
    "    add_embeddings_to_collection(db, \"interviews\", batch_size=5)\n",
    "    add_embeddings_to_collection(db, \"workorders\", batch_size=10)\n",
    "else:\n",
    "    print(\"✗ Skipping embedding generation - API not configured or failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4025258",
   "metadata": {},
   "source": [
    "## 6: Create Vector Indexes in MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31cea92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vector_search_index(db, collection_name: str, embedding_dim: int = 1024) -> bool:\n",
    "    \"\"\"\n",
    "    Create a vector search index on the embeddings field\n",
    "    \n",
    "    Note: This requires MongoDB Atlas with Atlas Vector Search enabled\n",
    "    \n",
    "    Args:\n",
    "        db: MongoDB database object\n",
    "        collection_name: Name of the collection\n",
    "        embedding_dim: Dimension of the embeddings\n",
    "        \n",
    "    Returns:\n",
    "        True if successful, False otherwise\n",
    "    \"\"\"\n",
    "    try:\n",
    "        collection = db[collection_name]\n",
    "        \n",
    "        # Vector search index definition for Atlas Vector Search\n",
    "\n",
    "\n",
    "        search_index_model = SearchIndexModel(\n",
    "                                    definition={\n",
    "                                        \"fields\": [\n",
    "                                        {\n",
    "                                            \"type\": \"vector\",\n",
    "                                            \"path\": \"embeddings\",\n",
    "                                            \"numDimensions\": embedding_dim,\n",
    "                                            \"similarity\": \"cosine\"\n",
    "                                        }\n",
    "                                        ]\n",
    "                                    },\n",
    "                                    name=\"vector_index\",\n",
    "                                    type=\"vectorSearch\"\n",
    "                                )\n",
    "\n",
    "\n",
    "        \n",
    "        # Create the index via the collection's create_search_indexes method\n",
    "        # Note: This method requires MongoDB Python driver >= 4.6\n",
    "        try:\n",
    "            # Try using the newer search indexes API\n",
    "            search_indexes = collection.list_search_indexes()\n",
    "            existing_indexes = [idx.get('name') for idx in search_indexes]\n",
    "            for index in existing_indexes:\n",
    "                print(index)\n",
    "            \n",
    "            if 'vector_index' not in existing_indexes:\n",
    "                collection.create_search_index(model=search_index_model)\n",
    "                print(f\"✓ Created vector search index for '{collection_name}'\")\n",
    "            else:\n",
    "                print(f\"✓ Vector search index already exists for '{collection_name}'\")\n",
    "                \n",
    "        except AttributeError:\n",
    "            # Fallback for older driver versions\n",
    "            print(f\"⚠ Vector search index creation requires MongoDB Atlas with Vector Search enabled\")\n",
    "            print(f\"  Manually create the index in MongoDB Atlas UI with this definition:\")\n",
    "            print(f\"  {json.dumps(index_definition, indent=2)}\")\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"⚠ Note: {e}\")\n",
    "        print(f\"  Vector indexes should be created in MongoDB Atlas UI\")\n",
    "        return False\n",
    "\n",
    "# Create vector indexes for both collections\n",
    "if MONGODB_URI and test_embeddings:\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"CREATING VECTOR SEARCH INDEXES\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Use embedding dimension from test\n",
    "    embedding_dim = len(test_embeddings[0]) if test_embeddings else 1024\n",
    "    \n",
    "    create_vector_search_index(db, \"manuals\", embedding_dim)\n",
    "    create_vector_search_index(db, \"interviews\", embedding_dim)\n",
    "    create_vector_search_index(db, \"workorders\", embedding_dim)\n",
    "else:\n",
    "    print(\"✗ Skipping index creation - prerequisites not met\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef8e2cf",
   "metadata": {},
   "source": [
    "## 7: Implement RAG Solution with OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75e335e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_search_mongodb(db, collection_name: str, query: str, num_results: int = 5) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Perform vector similarity search on MongoDB collection\n",
    "    \n",
    "    Args:\n",
    "        db: MongoDB database object\n",
    "        collection_name: Name of the collection to search\n",
    "        query: Query in natural language\n",
    "        num_results: Number of results to return\n",
    "        \n",
    "    Returns:\n",
    "        List of matching documents with similarity scores\n",
    "    \"\"\"\n",
    "    try:\n",
    "        collection = db[collection_name]\n",
    "\n",
    "        query_embedding = generate_embeddings_batch([query])\n",
    "        query_vector = query_embedding[0]\n",
    "\n",
    "        \n",
    "        # Use aggregation pipeline with vector search\n",
    "        pipeline = [\n",
    "            {\n",
    "                \"$vectorSearch\": {\n",
    "                    \"index\": \"vector_index\",\n",
    "                    'queryVector': query_vector,\n",
    "                    'numCandidates': 10, \n",
    "                    'limit': num_results\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"$project\": {\n",
    "                    \"'score\": {\"$meta\": \"vectorSearchScore\"},\n",
    "                    \"document\": \"$$ROOT\"\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"$limit\": num_results\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        # Try standard vector search first\n",
    "        try:\n",
    "            results = list(collection.aggregate(pipeline))\n",
    "            return results\n",
    "        except:\n",
    "            # Fallback to simpler approach if aggregation fails\n",
    "            # This works with documents that have embeddings field\n",
    "            results = []\n",
    "            documents = list(collection.find({\"embeddings\": {\"$exists\": True}}))\n",
    "            \n",
    "            if not documents:\n",
    "                return []\n",
    "            \n",
    "            # Calculate similarity scores using cosine similarity\n",
    "            import numpy as np\n",
    "            query_vec = np.array(query_vector)\n",
    "            \n",
    "            for doc in documents:\n",
    "                if 'embeddings' in doc:\n",
    "                    doc_vec = np.array(doc['embeddings'])\n",
    "                    # Cosine similarity\n",
    "                    similarity = np.dot(query_vec, doc_vec) / (np.linalg.norm(query_vec) * np.linalg.norm(doc_vec))\n",
    "                    results.append({\n",
    "                        'similarityScore': float(similarity),\n",
    "                        'document': doc\n",
    "                    })\n",
    "            \n",
    "            # Sort by similarity score and return top results\n",
    "            results.sort(key=lambda x: x['similarityScore'], reverse=True)\n",
    "            return results[:num_results]\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error performing vector search: {e}\")\n",
    "        return []\n",
    "\n",
    "def retrieve_context(db, query: str, num_results: int = 3) -> str:\n",
    "    \"\"\"\n",
    "    Retrieve relevant context from both collections using vector search\n",
    "    \n",
    "    Args:\n",
    "        db: MongoDB database object\n",
    "        query: User query\n",
    "        num_results: Number of results per collection\n",
    "        \n",
    "    Returns:\n",
    "        Formatted context string for RAG\n",
    "    \"\"\"\n",
    "    # Generate query embedding\n",
    "    \n",
    "    if not query:\n",
    "        return \"No context available\"\n",
    "        \n",
    "    # Search both collections\n",
    "    manual_results = vector_search_mongodb(db, \"manuals\", query, num_results)\n",
    "    interview_results = vector_search_mongodb(db, \"interviews\", query, num_results)\n",
    "    workorder_results = vector_search_mongodb(db, \"workorders\", query, num_results)\n",
    "\n",
    "    # Format context\n",
    "    context = \"Retrieved Context:\\n\\n\"\n",
    "    \n",
    "    if manual_results:\n",
    "        context += \"=== From Manuals ===\\n\"\n",
    "        for i, result in enumerate(manual_results, 1):\n",
    "            doc = result.get('document', result)\n",
    "            score = result.get('similarityScore', 0)\n",
    "            text = extract_text_for_embedding(doc)[:500]  # Limit text length\n",
    "            context += f\"{i}. (Score: {score:.2f}) {text}...\\n\\n\"\n",
    "    \n",
    "    if interview_results:\n",
    "        context += \"=== From Interviews ===\\n\"\n",
    "        for i, result in enumerate(interview_results, 1):\n",
    "            doc = result.get('document', result)\n",
    "            score = result.get('similarityScore', 0)\n",
    "            text = extract_text_for_embedding(doc)[:500]  # Limit text length\n",
    "            context += f\"{i}. (Score: {score:.2f}) {text}...\\n\\n\"\n",
    "\n",
    "    if workorder_results:\n",
    "        context += \"=== From Work Orders ===\\n\"\n",
    "        for i, result in enumerate(workorder_results, 1):\n",
    "            doc = result.get('document', result)\n",
    "            score = result.get('similarityScore', 0)\n",
    "            text = extract_text_for_embedding(doc)[:500]  # Limit text length\n",
    "            context += f\"{i}. (Score: {score:.2f}) {text}...\\n\\n\"\n",
    "    \n",
    "    return context\n",
    "\n",
    "# Initialize RAG system\n",
    "class MongoDBOpenAIRAG:\n",
    "    \"\"\"RAG system using MongoDB Atlas and OpenAI-compatible LLMs\"\"\"\n",
    "    \n",
    "    def __init__(self, db, model: str = None, temperature: float = 0.7):\n",
    "        self.db = db\n",
    "        self.model = model if model else COMPLETION_MODEL  # Use env var if not specified\n",
    "        self.temperature = temperature\n",
    "    \n",
    "    def answer_question(self, query: str, num_context_docs: int = 3) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Answer a question using RAG approach\n",
    "        \n",
    "        Args:\n",
    "            query: User question\n",
    "            num_context_docs: Number of context documents to retrieve\n",
    "            \n",
    "        Returns:\n",
    "            Dict with answer, context, and sources\n",
    "        \"\"\"\n",
    "        # Retrieve context\n",
    "        context = retrieve_context(self.db, query, num_context_docs)\n",
    "        \n",
    "        # Create prompt for OpenAI\n",
    "        system_prompt = \"\"\"You are a helpful assistant answering questions about maintenance systems and procedures.\n",
    "Use the provided context to answer the question accurately. If the context doesn't contain relevant information, say so.\n",
    "Always cite your sources from the context.\"\"\"\n",
    "        \n",
    "        user_message = f\"\"\"Context Information:\n",
    "{context}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Please provide a helpful answer based on the context above.\"\"\"\n",
    "        \n",
    "        try:\n",
    "            # Call OpenAI API\n",
    "            response = llm_client.chat.completions.create(\n",
    "                model=self.model,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": system_prompt},\n",
    "                    {\"role\": \"user\", \"content\": user_message}\n",
    "                ],\n",
    "                temperature=self.temperature,\n",
    "                max_tokens=500\n",
    "            )\n",
    "\n",
    "            answer = response.choices[0].message.content\n",
    "\n",
    "            return {\n",
    "                'success': True,\n",
    "                'query': query,\n",
    "                'answer': answer,\n",
    "                'context': context,\n",
    "                'model': self.model\n",
    "            }\n",
    "        except Exception as e:\n",
    "            return {\n",
    "                'success': False,\n",
    "                'query': query,\n",
    "                'error': str(e),\n",
    "                'context': context\n",
    "            }\n",
    "\n",
    "# Initialize RAG system\n",
    "if MONGODB_URI and LLM_API_KEY:\n",
    "    rag_system = MongoDBOpenAIRAG(db, model=COMPLETION_MODEL)\n",
    "    print(f\"✓ RAG system initialized successfully with model: {COMPLETION_MODEL}\")\n",
    "else:\n",
    "    print(\"✗ RAG system initialization failed - missing API keys\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb73cfa0",
   "metadata": {},
   "source": [
    "## 8: Query and Retrieve Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb75ec94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_rag_response(response: Dict[str, Any]) -> str:\n",
    "    \"\"\"\n",
    "    Format RAG response for display\n",
    "    \n",
    "    Args:\n",
    "        response: Response dictionary from RAG system\n",
    "        \n",
    "    Returns:\n",
    "        Formatted string for display\n",
    "    \"\"\"\n",
    "    output = \"\\n\" + \"=\" * 70 + \"\\n\"\n",
    "    output += f\"QUERY: {response.get('query', 'N/A')}\\n\"\n",
    "    output += \"=\" * 70 + \"\\n\\n\"\n",
    "    \n",
    "    if response.get('success'):\n",
    "        output += f\"ANSWER:\\n{response.get('answer', 'No answer generated')}\\n\\n\"\n",
    "        output += \"-\" * 70 + \"\\n\"\n",
    "        output += f\"CONTEXT SOURCES:\\n{response.get('context', 'No context retrieved')}\\n\"\n",
    "    else:\n",
    "        output += f\"ERROR: {response.get('error', 'Unknown error')}\\n\"\n",
    "        output += f\"RETRIEVED CONTEXT:\\n{response.get('context', 'No context')}\\n\"\n",
    "    \n",
    "    output += \"=\" * 70 + \"\\n\"\n",
    "    return output\n",
    "\n",
    "# Example queries to test the RAG system\n",
    "example_queries = [\n",
    "    \"What are the maintenance procedures for critical equipment?\",\n",
    "    \"E12 high temperature\",\n",
    "    \"What is the recommended maintenance schedule?\"\n",
    "]\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"RAG SYSTEM DEMONSTRATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "if 'rag_system' in locals() and MONGODB_URI:\n",
    "    print(\"\\nRunning example queries...\\n\")\n",
    "    \n",
    "    for i, query in enumerate(example_queries, 1):\n",
    "        print(f\"\\n--- Query {i} ---\")\n",
    "        response = rag_system.answer_question(query, num_context_docs=2)\n",
    "        formatted = format_rag_response(response)\n",
    "        print(formatted)\n",
    "        \n",
    "        # Add a small delay between API calls to avoid rate limiting\n",
    "        import time\n",
    "        if i < len(example_queries):\n",
    "            time.sleep(2)\n",
    "else:\n",
    "    print(\"\\n✗ RAG system not available for querying\")\n",
    "    print(\"  Ensure MONGODB_URI and OPENAI_API_KEY are configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b62c0f",
   "metadata": {},
   "source": [
    "## 9. Import Required Libraries\n",
    "\n",
    "Import necessary libraries including langchain, langgraph, and other dependencies for building the failure agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7c552f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Required Libraries\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "from typing import Any, Dict, Optional, List\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# LangChain and LangGraph imports\n",
    "from langchain_core.tools import tool\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, ToolMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.types import StateSnapshot\n",
    "from langgraph.graph.message import add_messages\n",
    "from typing import Annotated\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "027d3d3c",
   "metadata": {},
   "source": [
    "## 10. Define the State Schema\n",
    "\n",
    "The state schema maintains the conversation history and messages throughout the agent's execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7337f6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define State Schema\n",
    "from typing import TypedDict\n",
    "\n",
    "class FailureAgentState(TypedDict):\n",
    "    \"\"\"State schema for the Failure Agent\"\"\"\n",
    "    messages: Annotated[List[BaseMessage], add_messages]\n",
    "    \n",
    "print(\"✓ State schema defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c4374c",
   "metadata": {},
   "source": [
    "## 11. Create Tool Functions\n",
    "\n",
    "The failure agent uses four main tools to diagnose failures and generate incident reports:\n",
    "- **retrieve_manual**: Search technical manuals for relevant information\n",
    "- **retrieve_work_orders**: Find related maintenance work orders\n",
    "- **retrieve_interviews**: Access maintenance staff expertise and historical insights\n",
    "- **generate_incident_report**: Create and store incident reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6d6d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== IMPROVED TOOL DEFINITIONS =====\n",
    "\n",
    "def insert_incident_report(\n",
    "    error_code: str,\n",
    "    error_name: str,\n",
    "    root_cause: str,\n",
    "    repair_instructions: List[Dict[str, Any]],\n",
    "    machine_id: str,\n",
    "    timestamp: Optional[str] = None,\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Insert an incident report into MongoDB.\n",
    "    \n",
    "    Args:\n",
    "        error_code: The error code\n",
    "        error_name: Human-readable error name\n",
    "        root_cause: Root cause analysis\n",
    "        repair_instructions: List of repair steps with step and description keys\n",
    "        machine_id: Machine ID\n",
    "        timestamp: Optional timestamp\n",
    "        \n",
    "    Returns:\n",
    "        Dict with success status and incident ID\n",
    "    \"\"\"\n",
    "    from datetime import datetime, timezone\n",
    "    \n",
    "    try:\n",
    "        # Validate repair instructions\n",
    "        if not isinstance(repair_instructions, list) or len(repair_instructions) == 0:\n",
    "            raise ValueError(\"repair_instructions must be a non-empty list\")\n",
    "        \n",
    "        # Ensure each instruction has required fields\n",
    "        validated_instructions = []\n",
    "        for i, instruction in enumerate(repair_instructions):\n",
    "            if isinstance(instruction, str):\n",
    "                validated_instructions.append({\"step\": i + 1, \"description\": instruction})\n",
    "            elif isinstance(instruction, dict):\n",
    "                if \"description\" not in instruction:\n",
    "                    instruction[\"description\"] = str(instruction)\n",
    "                if \"step\" not in instruction:\n",
    "                    instruction[\"step\"] = i + 1\n",
    "                validated_instructions.append(instruction)\n",
    "            else:\n",
    "                validated_instructions.append({\"step\": i + 1, \"description\": str(instruction)})\n",
    "        \n",
    "        collection = db[\"incident_reports\"]\n",
    "        \n",
    "        report = {\n",
    "            \"error_code\": error_code,\n",
    "            \"error_name\": error_name,\n",
    "            \"root_cause\": root_cause,\n",
    "            \"repair_instructions\": validated_instructions,\n",
    "            \"machine_id\": machine_id,\n",
    "            \"timestamp\": timestamp or datetime.now(timezone.utc).isoformat(),\n",
    "            \"status\": \"created\",\n",
    "            \"created_at\": datetime.now(timezone.utc).isoformat(),\n",
    "        }\n",
    "        \n",
    "        result = collection.insert_one(report)\n",
    "        \n",
    "        return {\n",
    "            \"success\": True,\n",
    "            \"incident_id\": str(result.inserted_id),\n",
    "            \"message\": \"Incident report created successfully\",\n",
    "            \"error_code\": error_code,\n",
    "            \"machine_id\": machine_id\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"success\": False,\n",
    "            \"error\": str(e),\n",
    "            \"message\": f\"Failed to insert incident report: {e}\"\n",
    "        }\n",
    "\n",
    "\n",
    "@tool\n",
    "def retrieve_manual(query: str, n: int = 3) -> str:\n",
    "    \"\"\"\n",
    "    Retrieve relevant technical manuals for the alert via MongoDB vector search.\n",
    "    \n",
    "    Args:\n",
    "        query: The search query for technical documentation\n",
    "        n: Number of results to return (default 3)\n",
    "    \n",
    "    Returns:\n",
    "        JSON string containing relevant manual excerpts\n",
    "    \"\"\"\n",
    "    try:\n",
    "        results = vector_search_mongodb(db, \"manuals\", query, n)\n",
    "        if not results:\n",
    "            return json.dumps({\"found\": False, \"message\": \"No manuals found for query\"})\n",
    "        \n",
    "        formatted_results = []\n",
    "        for result in results:\n",
    "            doc = result.get('document', result)\n",
    "            score = result.get('similarityScore', 0)\n",
    "            text = extract_text_for_embedding(doc)[:300]\n",
    "            formatted_results.append({\n",
    "                \"score\": round(score, 3),\n",
    "                \"content\": text\n",
    "            })\n",
    "        \n",
    "        return json.dumps({\"found\": True, \"count\": len(formatted_results), \"results\": formatted_results})\n",
    "    except Exception as e:\n",
    "        return json.dumps({\"error\": str(e), \"found\": False})\n",
    "\n",
    "\n",
    "@tool\n",
    "def retrieve_work_orders(query: str, n: int = 3) -> str:\n",
    "    \"\"\"\n",
    "    Retrieve related work orders for the alert via MongoDB vector search.\n",
    "    \n",
    "    Args:\n",
    "        query: The search query for work orders\n",
    "        n: Number of results to return (default 3)\n",
    "    \n",
    "    Returns:\n",
    "        JSON string containing related work order information\n",
    "    \"\"\"\n",
    "    try:\n",
    "        results = vector_search_mongodb(db, \"workorders\", query, n)\n",
    "        if not results:\n",
    "            return json.dumps({\"found\": False, \"message\": \"No work orders found for query\"})\n",
    "        \n",
    "        formatted_results = []\n",
    "        for result in results:\n",
    "            doc = result.get('document', result)\n",
    "            score = result.get('similarityScore', 0)\n",
    "            text = extract_text_for_embedding(doc)[:300]\n",
    "            formatted_results.append({\n",
    "                \"score\": round(score, 3),\n",
    "                \"content\": text\n",
    "            })\n",
    "        \n",
    "        return json.dumps({\"found\": True, \"count\": len(formatted_results), \"results\": formatted_results})\n",
    "    except Exception as e:\n",
    "        return json.dumps({\"error\": str(e), \"found\": False})\n",
    "\n",
    "\n",
    "@tool\n",
    "def retrieve_interviews(query: str, n: int = 3) -> str:\n",
    "    \"\"\"\n",
    "    Retrieve interviews and expertise related to the alert via MongoDB vector search.\n",
    "    \n",
    "    Args:\n",
    "        query: The search query for maintenance expertise\n",
    "        n: Number of results to return (default 3)\n",
    "    \n",
    "    Returns:\n",
    "        JSON string containing relevant interview excerpts\n",
    "    \"\"\"\n",
    "    try:\n",
    "        results = vector_search_mongodb(db, \"interviews\", query, n)\n",
    "        if not results:\n",
    "            return json.dumps({\"found\": False, \"message\": \"No interviews found for query\"})\n",
    "        \n",
    "        formatted_results = []\n",
    "        for result in results:\n",
    "            doc = result.get('document', result)\n",
    "            score = result.get('similarityScore', 0)\n",
    "            text = extract_text_for_embedding(doc)[:300]\n",
    "            formatted_results.append({\n",
    "                \"score\": round(score, 3),\n",
    "                \"content\": text\n",
    "            })\n",
    "        \n",
    "        return json.dumps({\"found\": True, \"count\": len(formatted_results), \"results\": formatted_results})\n",
    "    except Exception as e:\n",
    "        return json.dumps({\"error\": str(e), \"found\": False})\n",
    "\n",
    "\n",
    "@tool\n",
    "def generate_incident_report(\n",
    "    error_code: str,\n",
    "    error_name: str,\n",
    "    root_cause: str,\n",
    "    repair_instructions: List[str],\n",
    "    machine_id: str\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Generate and store an incident report in MongoDB for the failure alert.\n",
    "    \n",
    "    Args:\n",
    "        error_code: The error code for the incident (e.g., \"E12\")\n",
    "        error_name: Human-readable name of the error (e.g., \"High temperature\")\n",
    "        root_cause: Root cause analysis inferred from context (detailed explanation)\n",
    "        repair_instructions: List of repair step descriptions as strings (e.g., [\"Step 1: Stop the motor\", \"Step 2: Inspect bearing\"])\n",
    "        machine_id: ID of the affected machine\n",
    "    \n",
    "    Returns:\n",
    "        JSON string with incident report confirmation\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Convert string list to dict list with step numbers\n",
    "        instructions_with_steps = []\n",
    "        for i, instruction in enumerate(repair_instructions, 1):\n",
    "            instructions_with_steps.append({\n",
    "                \"step\": i,\n",
    "                \"description\": str(instruction)\n",
    "            })\n",
    "        \n",
    "        result = insert_incident_report(\n",
    "            error_code=error_code,\n",
    "            error_name=error_name,\n",
    "            root_cause=root_cause,\n",
    "            repair_instructions=instructions_with_steps,\n",
    "            machine_id=machine_id\n",
    "        )\n",
    "        return json.dumps(result)\n",
    "    except Exception as e:\n",
    "        return json.dumps({\n",
    "            \"success\": False,\n",
    "            \"error\": str(e),\n",
    "            \"message\": f\"Error creating incident report: {e}\"\n",
    "        })\n",
    "\n",
    "\n",
    "\n",
    "# Get all tools\n",
    "tools = [retrieve_manual, retrieve_work_orders, retrieve_interviews, generate_incident_report]\n",
    "\n",
    "print(\"✓ Tool functions defined and registered (using MongoDB + Voyage AI)\")\n",
    "\n",
    "# Initialize the LLM\n",
    "llm = ChatOpenAI(\n",
    "    model=os.getenv(\"COMPLETION_MODEL\"),\n",
    "    temperature=0,\n",
    "    api_key=os.getenv(\"LLM_API_KEY\")\n",
    ")\n",
    "\n",
    "# Bind tools to the model\n",
    "llm_with_tools = llm.bind_tools(tools)\n",
    "\n",
    "print(\"✓ Language model configured with tools\")\n",
    "\n",
    "# Test the generate_incident_report tool directly\n",
    "test_report = insert_incident_report(\n",
    "    error_code=\"E12\",\n",
    "    error_name=\"High temperature\",\n",
    "    root_cause=\"Bearing friction due to insufficient lubrication\",\n",
    "    repair_instructions=[\n",
    "        {\"step\": 1, \"description\": \"Stop the motor and let it cool\"},\n",
    "        {\"step\": 2, \"description\": \"Inspect bearing for damage\"},\n",
    "        {\"step\": 3, \"description\": \"Apply fresh lubricant\"},\n",
    "        {\"step\": 4, \"description\": \"Restart and monitor temperature\"}\n",
    "    ],\n",
    "    machine_id=\"M1\"\n",
    ")\n",
    "\n",
    "print(\"✓ Tool test result:\")\n",
    "print(json.dumps(test_report, indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc3ffdcb",
   "metadata": {},
   "source": [
    "12. Define Agent Nodes and Routing Logic\n",
    "Agent Node\n",
    "The agent node processes incoming messages and calls the LLM to determine the next action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59e9a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Agent Node\n",
    "async def agent_node(state: FailureAgentState) -> FailureAgentState:\n",
    "    \"\"\"\n",
    "    The agent node:\n",
    "    1. Receives alert details about machine failures\n",
    "    2. Routes to appropriate tools for information gathering\n",
    "    3. Analyzes retrieved context\n",
    "    4. Decides when to generate incident report\n",
    "    \"\"\"\n",
    "    # Create the prompt template\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"You are the Failure Agent. Your role is to:\n",
    "1. Receive alert details about machine failures\n",
    "2. Retrieve additional context from manuals, work orders, and maintenance expertise\n",
    "3. Analyze the root cause of the failure\n",
    "4. Generate a comprehensive incident report with repair instructions\n",
    "\n",
    "IMPORTANT: When calling generate_incident_report, you MUST provide:\n",
    "- error_code: The error code from the alert\n",
    "- error_name: The error name from the alert\n",
    "- root_cause: Your analysis of why this failure occurred based on retrieved context\n",
    "- repair_instructions: A LIST OF STRINGS describing step-by-step repair procedures (required - never omit!)\n",
    "- machine_id: The machine ID from the alert\n",
    "\n",
    "Example repair_instructions format:\n",
    "[\"Stop the motor and allow it to cool for 30 minutes\", \"Inspect the bearing for wear and damage\", \"Apply fresh lubricant to bearings\", \"Restart motor and monitor temperature\"]\n",
    "\n",
    "Use your tools strategically to gather all necessary information before generating the incident report.\n",
    "After the incident report is generated, acknowledge the completion with a brief summary.\"\"\"\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ])\n",
    "    \n",
    "    # Format the messages\n",
    "    formatted_prompt = await prompt.ainvoke({\"messages\": state[\"messages\"]})\n",
    "    \n",
    "    # Get the response from the model\n",
    "    response = await llm_with_tools.ainvoke(formatted_prompt)\n",
    "    \n",
    "    return {\n",
    "        \"messages\": [response]\n",
    "    }\n",
    "\n",
    "print(\"✓ Agent node defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "050c8bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Tool Execution Node\n",
    "async def process_tool_calls(state: FailureAgentState) -> FailureAgentState:\n",
    "    \"\"\"\n",
    "    Process tool calls from the agent and return the results.\n",
    "    \"\"\"\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "    \n",
    "    tool_results = []\n",
    "    \n",
    "    # Check if the last message has tool calls\n",
    "    if hasattr(last_message, \"tool_calls\") and last_message.tool_calls:\n",
    "        for tool_call in last_message.tool_calls:\n",
    "            tool_name = tool_call[\"name\"]\n",
    "            tool_input = tool_call[\"args\"]\n",
    "            \n",
    "            print(f\"\\n🔧 Executing tool: {tool_name}\")\n",
    "            print(f\"   Input: {tool_input}\")\n",
    "            \n",
    "            # Find and execute the tool\n",
    "            for tool in tools:\n",
    "                if tool.name == tool_name:\n",
    "                    result = await tool.ainvoke(tool_input)\n",
    "                    print(f\"   Result: {result[:100]}...\")\n",
    "                    \n",
    "                    tool_message = ToolMessage(\n",
    "                        content=result,\n",
    "                        tool_call_id=tool_call[\"id\"]\n",
    "                    )\n",
    "                    tool_results.append(tool_message)\n",
    "                    break\n",
    "    \n",
    "    return {\n",
    "        \"messages\": tool_results\n",
    "    }\n",
    "\n",
    "print(\"✓ Tool execution node defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2c669a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Routing Logic\n",
    "def should_continue(state: FailureAgentState) -> str:\n",
    "    \"\"\"\n",
    "    Routes to 'tools' node if tool calls present\n",
    "    Returns END otherwise to terminate agent loop\n",
    "    \"\"\"\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "    \n",
    "    # If the last message has tool calls, route to the tools node\n",
    "    if hasattr(last_message, \"tool_calls\") and last_message.tool_calls:\n",
    "        return \"tools\"\n",
    "    \n",
    "    # Otherwise, end the agent\n",
    "    return END\n",
    "\n",
    "print(\"✓ Routing logic defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "215cb00e",
   "metadata": {},
   "source": [
    "## 13. Compile the Graph\n",
    "\n",
    "Create the StateGraph and compile it into an executable agent.\n",
    "Creates a StateGraph with proper node connections:\n",
    "\n",
    "Graph Structure:\n",
    "START → agent → [decision]\n",
    "                    ├─→ tools → agent (loop back)\n",
    "                    └─→ END\n",
    "\n",
    "Compilation:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39c9f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the StateGraph\n",
    "workflow = StateGraph(FailureAgentState)\n",
    "\n",
    "# Add nodes\n",
    "workflow.add_node(\"agent\", agent_node)\n",
    "workflow.add_node(\"tools\", process_tool_calls)\n",
    "\n",
    "# Add edges\n",
    "workflow.add_edge(START, \"agent\")\n",
    "workflow.add_conditional_edges(\"agent\", should_continue)\n",
    "workflow.add_edge(\"tools\", \"agent\")\n",
    "\n",
    "# Compile the graph\n",
    "failure_agent = workflow.compile()\n",
    "\n",
    "print(\"✓ Failure Agent graph compiled successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca24513",
   "metadata": {},
   "source": [
    "## 14. Test Scenario: Motor Overheating Alert (E12)\n",
    "Alert Input\n",
    "\n",
    "{\n",
    "  \"err_code\": \"E12\",\n",
    "  \"err_name\": \"High temperature\",\n",
    "  \"machine_id\": \"M1\",\n",
    "  \"details\": {\n",
    "    \"temperature\": 114.55,\n",
    "    \"vibration\": 0.235\n",
    "  },\n",
    "  \"ts\": \"2026-01-26T01:01:04.980Z\",\n",
    "  \"status\": \"new\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610f96f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Scenario 1: Motor Overheating\n",
    "print(\"=\" * 80)\n",
    "print(\"TEST SCENARIO 1: MOTOR OVERHEATING (E001)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "test_input_1 = \"\"\"\n",
    "{ \"err_code\": \"E12\",  \n",
    "  \"err_name\": \"High temperature\",  \n",
    "  \"machine_id\": \"M1\",  \n",
    "  \"details\": {  \"temperature\": 114.55,    \n",
    "                \"vibration\": 0.235  },  \n",
    "                \"ts\": {    \"$date\": \"2026-01-26T01:01:04.980Z\"  },  \n",
    "  \"status\": \"new\"}\n",
    "\"\"\"\n",
    "\n",
    "initial_state_1 = {\n",
    "    \"messages\": [HumanMessage(content=test_input_1)]\n",
    "}\n",
    "\n",
    "print(\"\\n📨 Input Alert:\")\n",
    "print(test_input_1)\n",
    "print(\"\\n🤖 Agent Processing...\")\n",
    "\n",
    "\n",
    "# Run the agent asynchronously\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "\n",
    "# Apply nest_asyncio to allow nested event loops in Jupyter\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Run the agent directly without wrapping\n",
    "result = asyncio.run(failure_agent.ainvoke(initial_state_1))\n",
    "\n",
    "\n",
    "print(\"\\n✅ Agent Completed Processing.\")\n",
    "print(\"\\n📋 Final State Messages:\")\n",
    "for msg in result.get(\"messages\", []):\n",
    "    print(f\"  - {type(msg).__name__}: {str(msg.content)[:100]}...\")\n",
    "\n",
    "print(\"\\n✅ Agent Completed Processing.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
